<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors">
  <meta name="keywords" content="motion capture, virtual reality, full-body virtual, representation, kinematic chain, progressive estimation, neural
  network, IMU sensors">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors</title>

  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Zunjie Zhu<sup>1,2</sup>,
            </span>
            <span class="author-block">
              Yan Zhao<sup>3</sup>,
            </span>
            <span class="author-block">
              Yihan Hu<sup>1</sup>,
            </span>
            <span class="author-block">
              Guoxiang Wang<sup>4</sup>,
            </span>
            <span class="author-block">
              Hai Qiu<sup>5</sup>,
            </span>
            <span class="author-block">
              Bolun Zheng<sup>1</sup>,
            </span>
            <span class="author-block">
              Chenggang Yan<sup>1,6</sup>,
            </span>
            <span class="author-block">
              Feng Xu<sup>7</sup>,
            </span>
          </div>
          

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Hangzhou Dianzi University </span>
            <span class="author-block"><sup>2</sup>Wenzhou Institute of Hangzhou Dianzi University </span>
            <span class="author-block"><sup>3</sup>Tiangong University </span>
            <span class="author-block"><sup>4</sup>Lishui University </span>
            <span class="author-block"><sup>5</sup>Costar Intelligent Optoelectronics Technology Co., Ltd </span>
            <span class="author-block"><sup>6</sup>Macao Polytechnic University </span>
            <span class="author-block"><sup>7</sup>Tsinghua University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2505.05336"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.05336"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- Your image here -->
          <img src="static/images/fig8.jpg" alt="MY ALT TEXT"/>
          <p>
            The motion capture system that supports full-body virtual representation is of key significance for virtual reality. Compared to vision-based systems, full-body pose estimation from sparse tracking signals is not limited by environmental conditions or recording range. However, previous works either face the challenge of wearing additional sensors on the pelvis and lower-body or rely on external visual sensors to obtain global positions of key joints. To improve the practicality of the technology for virtual reality applications, we estimate full-body poses using only inertial data obtained from three Inertial Measurement Unit (IMU) sensors worn on the head and wrists, thereby reducing the complexity of the hardware system. In this work, we propose a method called Progressive Inertial Poser (ProgIP) for human pose estimation, which combines neural network estimation with a human dynamics model, considers the hierarchical structure of the kinematic chain, and employs a multi-stage progressive network estimation with increased depth to reconstruct full-body motion in real time. The encoder combines Transformer Encoder and bidirectional LSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial sequence, while the decoder based on multi-layer perceptrons (MLPs) transforms high-dimensional features and accurately projects them onto Skinned Multi-Person Linear (SMPL) model parameters. Quantitative and qualitative experimental results on multiple public datasets show that our method outperforms state-of-the-art methods with the same inputs, and is comparable to recent works using six IMU sensors.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero progIP">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Video</h2>
      <!-- 视频播放器 -->
      <video id="progIP" controls style="width: 100%; height: auto;">
        <source src="./static/videos/progIP.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </div>
</section>

    
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">The Pipeline of ProgIP</h2>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/fig1.jpg" alt="MY ALT TEXT"/>
        <h2 class="content has-text-justified">
          
       </h2>
     </div>
    </div>
  </div>
</div>
</div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Quantitative Results</h2>
      <div class="item">
        <img src="static/images/table3.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <img src="static/images/table4.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <img src="static/images/table5.png" alt="MY ALT TEXT"/>
      </div>
      <h2 class="content has-text-justified">
      </h2>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Qualitative Results</h2>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/fig2.png" alt="MY ALT TEXT"/>
        <h2 class="content has-text-justified">
          
       </h2>
     </div>
    </div>
  </div>
</div>
</div>
</section>



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhu2025progressiveinertialposerprogressive,
        title={Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors}, 
        author={Zunjie Zhu and Yan Zhao and Yihan Hu and Guoxiang Wang and Hai Qiu and Bolun Zheng and Chenggang Yan and Feng Xu},
        year={2025},
        eprint={2505.05336},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2505.05336}, 
  }
}</code></pre>
  </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<script>
bulmaCarousel.attach('#results-carousel11', {
    slidesToScroll: 1,
    slidesToShow: 2,
    infinite: true,
    autoplay: false,
});
bulmaCarousel.attach('#results-carousel22', {
    slidesToScroll: 1,
    slidesToShow: 2,
    infinite: true,
    autoplay: false,
});
bulmaCarousel.attach('#results-carousel33', {
    slidesToScroll: 1,
    slidesToShow: 2,
    infinite: true,
    autoplay: false,
});
</script>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>